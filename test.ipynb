{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS5xCDH0EZcr"
   },
   "source": [
    "# E-prop training of dope RNN on hold task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5243,
     "status": "ok",
     "timestamp": 1743798818031,
     "user": {
      "displayName": "Rich Pang",
      "userId": "16990121332087627621"
     },
     "user_tz": 240
    },
    "id": "fxlqoLpVKTPz",
    "outputId": "00c18dd3-e92f-4209-8744-da8f0b316c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1\n",
      "Cuda available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from disp import set_plot\n",
    "\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Cuda available:', torch.cuda.is_available())\n",
    "\n",
    "device = 'cuda' if  torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)  # tested on cpu and cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ86oHavDZaH"
   },
   "source": [
    "### E-prop factorization of the loss gradient\n",
    "\n",
    "$$\\frac{dE}{dW_{ij}} = \\sum_t \\frac{dE}{d\\hat{z}_i^t} \\left[ \\frac{\\partial \\hat{z}_i^t}{\\partial W_{ij}} \\right]_{local} = \\sum_t \\frac{dE}{d\\hat{z}_i^t} e_{ij}^t$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "e_{ij}^t = \\left[ \\frac{\\partial \\hat{z}_i^t}{\\partial W_{ij}} \\right]_{local} = \\frac{\\partial \\hat{z}_i^t}{\\partial \\hat{x}_i^t} \\sum_{t' \\leq t}\\frac{\\partial \\hat{x}_i^t}{\\partial \\hat{x}_i^{t-1}} \\dots \\frac{\\partial \\hat{x}_i^{t'+1}}{\\partial \\hat{x}_i^{t'}}\n",
    "\\frac{\\partial \\hat{x}_i^{t'}}{\\partial W_{ij}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9tlMz20WaHc"
   },
   "source": [
    "Or, in vector format (note that $\\frac{dE}{d\\mathbf{w}_i}$, $\\mathbf{e}_i^t$, and $\\boldsymbol{\\epsilon}_i^t$ are all row vectors):\n",
    "\n",
    "$$\\frac{dE}{d\\mathbf{w}_i} = \\sum_t \\frac{dE}{d\\hat{z}_i^t} \\left[ \\frac{\\partial \\hat{z}_i^t}{\\partial \\mathbf{w}_i} \\right]_{local} = \\sum_t \\frac{dE}{d\\hat{z}_i^t} \\mathbf{e}_i^t = \\sum_t L_i^t \\mathbf{e}_i^t$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_i^t = \\left[ \\frac{\\partial \\hat{z}_i^t}{\\partial \\mathbf{w}_i} \\right]_{local} = \\frac{\\partial \\hat{z}_i^t}{\\partial \\hat{x}_i^t} \\sum_{t' \\leq t}\\frac{\\partial \\hat{x}_i^t}{\\partial \\hat{x}_i^{t-1}} \\dots \\frac{\\partial \\hat{x}_i^{t'+1}}{\\partial \\hat{x}_i^{t'}}\n",
    "\\frac{\\partial \\hat{x}_i^{t'}}{\\partial \\mathbf{w}_i}\n",
    "= \\frac{\\partial \\hat{z}_i^t}{\\partial \\hat{x}_i^t} \\boldsymbol{\\epsilon}_i^t\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\epsilon}_i^t \\equiv \\sum_{t' \\leq t}\\frac{\\partial \\hat{x}_i^t}{\\partial \\hat{x}_i^{t-1}} \\dots \\frac{\\partial \\hat{x}_i^{t'+1}}{\\partial \\hat{x}_i^{t'}}\n",
    "\\frac{\\partial \\hat{x}_i^{t'}}{\\partial \\mathbf{w}_i}\n",
    "$$\n",
    "\n",
    "which can also be computed recursively:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\epsilon}_i^1 = \\frac{\\partial \\hat{x}_i^1}{\\partial \\mathbf{w}_i} \\quad \\quad \\quad\n",
    "\\boldsymbol{\\epsilon}_i^t = \\frac{\\partial \\hat{x}_i^t}{\\partial \\hat{x}_i^{t-1}} \\boldsymbol{\\epsilon}_i^{t-1} + \\frac{\\partial \\hat{x}_i^t}{\\partial \\mathbf{w}_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1743798818034,
     "user": {
      "displayName": "Rich Pang",
      "userId": "16990121332087627621"
     },
     "user_tz": 240
    },
    "id": "7j40Ek0SwuvH"
   },
   "outputs": [],
   "source": [
    "# model params\n",
    "D = 1\n",
    "N = 2000\n",
    "TAU = .01\n",
    "G = 100\n",
    "\n",
    "DT = .001\n",
    "\n",
    "J_psi = G*torch.randn((N, D), device=device)\n",
    "B = torch.eye(D, device=device)\n",
    "C = torch.eye(D, device=device)\n",
    "\n",
    "# smln params\n",
    "x_0 = torch.tensor([[1.,]], device=device).T\n",
    "\n",
    "def get_z(x):\n",
    "    return 1*x\n",
    "\n",
    "def advance(x, z, w_T, u):\n",
    "    psi = (1 + torch.tanh(J_psi@z))*np.sqrt(2/N)\n",
    "    x_next = x + DT/TAU*(-x + C@w_T@psi + B@u)\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1743798818034,
     "user": {
      "displayName": "Rich Pang",
      "userId": "16990121332087627621"
     },
     "user_tz": 240
    },
    "id": "7j40Ek0SwuvH"
   },
   "outputs": [],
   "source": [
    "targs = [torch.tensor([1.], device=device), torch.tensor([-1.], device=device)]\n",
    "\n",
    "def loss_fn(z_hats, targ):\n",
    "    return torch.mean((z_hats[-1, :] - targ)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hlt5GL6WL1n"
   },
   "source": [
    "## E-prop\n",
    "\n",
    "We first study the ideal case (above), in which the factorization should reproduce the full BPTT gradient exactly. Set use_approx to True to see\n",
    "approx e-prop (where learning signal is replaced with partial derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "executionInfo": {
     "elapsed": 9399,
     "status": "ok",
     "timestamp": 1743798840301,
     "user": {
      "displayName": "Rich Pang",
      "userId": "16990121332087627621"
     },
     "user_tz": 240
    },
    "id": "qJ5tdeULhkEk",
    "outputId": "b09f6077-4374-4be5-930d-3e8beb818c61"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 2000] doesn't match the broadcast shape [1, 1, 2000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b18d7aca6808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0me_prop_grad_approx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0me_prop_grad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0me_prop_grad_approx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mLs_approx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 2000] doesn't match the broadcast shape [1, 1, 2000]"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "nepoch = 400\n",
    "# use_approx = False\n",
    "use_approx = True\n",
    "\n",
    "T = .06\n",
    "\n",
    "IT = int(round(T/DT))\n",
    "t = np.arange(0, IT+1, dtype=float)*DT\n",
    "\n",
    "w_T_hat = (0.*torch.ones((D, N), device=device)).detach()\n",
    "w_T_hat.requires_grad = True\n",
    "\n",
    "w_T_hat_flats = np.nan*np.ones((nepoch, D*N))\n",
    "\n",
    "z_hats_initial = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "# make inputs\n",
    "us = torch.zeros((IT+1, D), device=device)\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    # compute first terms (dE/dx_t) using autograd\n",
    "    x_hats = [x_0.clone()]\n",
    "    z_hats = [get_z(x_hats[0])]\n",
    "\n",
    "    for ct in range(1, IT+1):\n",
    "        x_hat = advance(x_hats[ct-1], z_hats[ct-1], w_T_hat, us[[ct], :].T)\n",
    "        x_hat.retain_grad()\n",
    "\n",
    "        z_hat = get_z(x_hat)\n",
    "        z_hat.retain_grad()\n",
    "\n",
    "        x_hats.append(x_hat)\n",
    "        z_hats.append(z_hat)\n",
    "\n",
    "    x_hats_cc = torch.cat(x_hats, dim=1).T\n",
    "    z_hats_cc = torch.cat(z_hats, dim=1).T\n",
    "    z_hats_cc.retain_grad()\n",
    "\n",
    "    loss = loss_fn(z_hats_cc, targs[0])\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # get backprop grad\n",
    "    bptt_grad = w_T_hat.grad.clone()\n",
    "\n",
    "    # get learning signals (x_hats grad)\n",
    "    Ls = [z_hat.grad for z_hat in z_hats]\n",
    "    Ls_approx = [grad for grad in z_hats_cc.grad]\n",
    "\n",
    "    w_T_hat.grad.zero_()\n",
    "\n",
    "    ## get eligibility vectors\n",
    "    evs = torch.zeros((IT+1, D, N), device=device)  # each row is an e-vector\n",
    "\n",
    "    for ct in range(1, IT+1):\n",
    "        partial_x_hat, _, partial_w_T_hat, _ = torch.autograd.functional.jacobian(advance, inputs=(x_hats[ct-1], z_hats[ct-1], w_T_hat, us[[ct], :].T))\n",
    "        # loop over neurons\n",
    "        for i in range(D):\n",
    "            evs[ct, i, :] = partial_x_hat[i, i]*evs[ct-1, i, :] + partial_w_T_hat[i][i, :]\n",
    "\n",
    "    # convert to eligibility traces\n",
    "    ets = 1*evs  # special case for identity activation function\n",
    "\n",
    "    # compute gradient\n",
    "    e_prop_grad = torch.zeros((D, N), device=device)\n",
    "    e_prop_grad_approx = torch.zeros((D, N), device=device)\n",
    "    for ct in range(1, IT+1):\n",
    "        e_prop_grad += Ls[ct][:, None]*ets[ct, i, :]\n",
    "        e_prop_grad_approx += Ls_approx[ct][:, None]*ets[ct, i, :]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_approx:\n",
    "            w_T_hat -= lr*e_prop_grad_approx\n",
    "        else:\n",
    "            w_T_hat -= lr*e_prop_grad\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    w_T_hat_flats[epoch, :] = w_T_hat.detach().cpu().numpy().flatten()\n",
    "\n",
    "    if epoch == 0:\n",
    "        z_hats_initial.append(z_hats_cc.detach().cpu().numpy())\n",
    "    elif epoch in [1, 5, 10]:\n",
    "        print('Epoch', epoch)\n",
    "        print('BPTT grad:', bptt_grad)\n",
    "        print('Eprop grad:', e_prop_grad)\n",
    "        print('Eprop grad approx:', e_prop_grad_approx)\n",
    "        \n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "axs[0].plot(losses)\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(w_T_hat_flats)\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('w_hat')\n",
    "\n",
    "axs[2].plot(t, z_hats_initial[0], c='b', ls='--')\n",
    "axs[2].plot(t, z_hats_stack.detach().cpu().numpy(), c='b', ls='-')\n",
    "axs[2].set_xlabel('Time (s)')\n",
    "axs[2].set_ylabel('z_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2.5, 2.5), tight_layout=True)\n",
    "ax.plot(losses)\n",
    "set_plot(ax, x_label='Epoch', y_label='Loss (1-R)')\n",
    "\n",
    "# if use_approx == False:\n",
    "#     fig.savefig('ms/5_hold_loss_eprop.svg')\n",
    "# else:\n",
    "#     fig.savefig('ms/5_hold_loss_eprop_approx.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOSzwNAxzRBE7FAL+qi6OH7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
